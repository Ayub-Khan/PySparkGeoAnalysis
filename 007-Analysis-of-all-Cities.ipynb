{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Analysis on All Features and All Cities\n",
    "\n",
    "OK we are now ready for the big moment of finding which city is the cultural capital of Europe based on our criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load './code/helpers/imports.py'\n",
    "import notebook\n",
    "import os.path, json, io, pandas\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (16, 20)\n",
    "\n",
    "from retrying import retry # for exponential back down when calling TurboOverdrive API\n",
    "\n",
    "import pyspark.sql.functions as func # resuse as func.coalace for example\n",
    "from pyspark.sql.types import StringType, IntegerType, FloatType, DoubleType,DecimalType\n",
    "\n",
    "import pandas as pandas\n",
    "from geopandas import GeoDataFrame # Loading boundaries Data\n",
    "from shapely.geometry import Point, Polygon, shape # creating geospatial data\n",
    "from shapely import wkb, wkt # creating and parsing geospatial data\n",
    "import overpy # OpenStreetMap API\n",
    "\n",
    "# make sure nbextensions are installed\n",
    "notebook.nbextensions.check_nbextension('usability/codefolding', user=True)\n",
    "\n",
    "try:\n",
    "    sc\n",
    "except NameError:\n",
    "    import pyspark\n",
    "    sc = pyspark.SparkContext('local[*]')\n",
    "    sqlContext = pyspark.sql.SQLContext(sc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load './code/helpers/load_boundaries_and_pois.py'\n",
    "OVERPASS_API         = overpy.Overpass()\n",
    "BASE_DIR             = os.path.join(os.path.abspath('.'), 'work-flow')\n",
    "URBAN_BOUNDARIES_FILE = '06_Europe_Cities_boundaries_with_Labels_Population.geo.json'\n",
    "\n",
    "# Paths to base datasets that we are using:\n",
    "URBAN_BOUNDARIES_PATH = os.path.join(BASE_DIR,URBAN_BOUNDARIES_FILE)\n",
    "POIS_PATH            = os.path.join(BASE_DIR, \"pois.json\")\n",
    "\n",
    "try:\n",
    "    geo_df\n",
    "except NameError:\n",
    "    geo_df = GeoDataFrame.from_file(URBAN_BOUNDARIES_PATH)\n",
    "    # Add a WKT column for use later\n",
    "    geo_df['wkt'] = pandas.Series(\n",
    "        map(lambda geom: str(geom.to_wkt()), geo_df['geometry']),\n",
    "        index=geo_df.index, dtype='string')\n",
    "\n",
    "try:\n",
    "    boundaries_from_pd\n",
    "except NameError:\n",
    "    boundaries_from_pd = sqlContext.createDataFrame(geo_df)\n",
    "    boundaries_from_pd.registerTempTable(\"boundaries\")\n",
    "\n",
    "try:\n",
    "    pois_df\n",
    "except NameError:\n",
    "    pois_df = sqlContext.read.json(POIS_PATH)\n",
    "    pois_df = pois_df.toPandas()\n",
    "    def toWktColumn(coords):\n",
    "        return (Point(coords).wkt)\n",
    "\n",
    "    pois_df['wkt'] = pandas.Series(\n",
    "        map(lambda geom: toWktColumn(geom.coordinates), pois_df['geometry']),\n",
    "        index=pois_df.index, dtype='string')\n",
    "\n",
    "    pois_df = sqlContext.createDataFrame(pois_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unique IDs\n",
    "as we saw in SpatialSpark each record in our dataset needs to have unique ID inorder to allow us to get the tuple matching our sptial predicates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonicallyIncreasingId\n",
    "\n",
    "# create dataframe with (id, geometry) for POIs\n",
    "# 1. Add and ID Column to POIs\n",
    "\n",
    "pois_df           = pois_df.withColumn(\"id\", monotonicallyIncreasingId())\n",
    "pois_tuple_id_wkt = pois_df.select(pois_df['id'], pois_df['wkt'])\n",
    "\n",
    "pois_tuple_id_wkt.show()\n",
    "pois_tuple_id_wkt.printSchema()\n",
    "print pois_tuple_id_wkt.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create dataframe with (id, geometry-as-WKT) for boundaries\n",
    "# 1. Add and ID Column to boundaries\n",
    "\n",
    "boundaries_from_pd     = boundaries_from_pd.withColumn(\"id\", monotonicallyIncreasingId())\n",
    "boundaries_tuple_id_wkt = boundaries_from_pd.select(boundaries_from_pd['id'], boundaries_from_pd['wkt'])\n",
    "\n",
    "boundaries_tuple_id_wkt.printSchema()\n",
    "print boundaries_from_pd.count()\n",
    "boundaries_tuple_id_wkt.show()\n",
    "wkt.loads(boundaries_tuple_id_wkt.take(7)[6].wkt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spatialspark = sc._jvm.spatialspark\n",
    "SpatialOperator      = spatialspark.operator.SpatialOperator \n",
    "BroadcastSpatialJoin = spatialspark.join.BroadcastSpatialJoin\n",
    "from ast import literal_eval as make_tuple # used to decode data from java\n",
    "\n",
    "joinPoiBdryRDD = BroadcastSpatialJoin.apply(sc._jsc, \n",
    "                                            pois_tuple_id_wkt._jdf, \n",
    "                                            boundaries_tuple_id_wkt._jdf, \n",
    "                                            SpatialOperator.Within(), \n",
    "                                            0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print joinPoiBdryRDD.count()\n",
    "\n",
    "joinResults = map(lambda result: make_tuple(result.toString()), joinPoiBdryRDD.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print pois_tuple_id_wkt.count()\n",
    "print boundaries_tuple_id_wkt.count()\n",
    "print joinResults[695]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make the results a DF\n",
    "rddResult = sc.parallelize(joinResults)\n",
    "df = sqlContext.createDataFrame(rddResult, [\"poi_id\", \"boundry_id\"])\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Do a join with poi df and bdry df\n",
    "df_with_pois = df.join(pois_df, df['poi_id'] == pois_df['id']).select(\n",
    "    df['poi_id'],\n",
    "    df['boundry_id'],\n",
    "    pois_df['properties'].alias(\"poi_properties\"),    \n",
    "    pois_df['wkt'].alias(\"poi_wkt\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_with_pois.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_with_pois_bdrys = df_with_pois.join(\n",
    "        boundaries_from_pd, df_with_pois['boundry_id'] == boundaries_from_pd['id']\n",
    "    ).select(\n",
    "        df_with_pois['poi_id'],\n",
    "        df_with_pois['boundry_id'],\n",
    "        df_with_pois['poi_properties'],\n",
    "        df_with_pois['poi_wkt'],\n",
    "        boundaries_from_pd['wkt'].alias(\"boundry_wkt\"),\n",
    "        boundaries_from_pd['NAMEASCII'].alias(\"city_name\"),\n",
    "        boundaries_from_pd['POPEU2013'].alias(\"population\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_with_pois_bdrys.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_with_pois_bdrys.cache()\n",
    "df_with_pois_bdrys.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_with_pois_bdrys.show(3)\n",
    "# we have now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ## what have we done so far\n",
    "\n",
    "# We have now created a join table with each POI and corresponding city along with the \n",
    "# population and city name POI's have a nested property thanks to geojson with keys for \n",
    "# the type of location. We will still need a way to decipher these\n",
    "\n",
    "# +------+----------+--------------------+--------------------+--------------------+---------+----------+\n",
    "# |poi_id|boundry_id|      poi_properties|             poi_wkt|         boundry_wkt|city_name|population|\n",
    "# +------+----------+--------------------+--------------------+--------------------+---------+----------+\n",
    "# |    31|         0|[null,null,null,n...|POINT (8.50017549...|MULTIPOLYGON (((8...|   Zurich|    380777|\n",
    "# |    32|         0|[null,null,null,n...|POINT (8.53091749...|MULTIPOLYGON (((8...|   Zurich|    380777|\n",
    "# |    33|         0|[null,null,null,n...|POINT (8.52973519...|MULTIPOLYGON (((8...|   Zurich|    380777|\n",
    "# +------+----------+--------------------+--------------------+--------------------+---------+----------+\n",
    "\n",
    "# The other issue is that the tag for location type is split across amentiy and tourism. \n",
    "# To simplify our our calculation we will create a colum for location type and have the label there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rec = df_with_pois_bdrys.take(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print rec.poi_properties.amenity or rec.poi_properties.tourism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For the first step we add columns for each tag. In this case we have amenity and tourism\n",
    "\n",
    "df_with_pois_bdrys = df_with_pois_bdrys.withColumn(\n",
    "        'tourism', df_with_pois_bdrys['poi_properties']['tourism']\n",
    "    ).withColumn(\n",
    "        'amenity', df_with_pois_bdrys['poi_properties']['amenity'])\n",
    "\n",
    "df_with_pois_bdrys.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We next coalesce the two columns into a single column called location_type and then group our data by\n",
    "# all the fields we need in this case city_name, population, location_type and the perform a count\n",
    "\n",
    "df = df_with_pois_bdrys.select('*', func.coalesce(\n",
    "        df_with_pois_bdrys['poi_properties']['tourism'], \n",
    "        df_with_pois_bdrys['poi_properties']['amenity']\n",
    "    ).alias(\"location_type\")).groupby('boundry_id', \n",
    "                                      'city_name', \n",
    "                                      'location_type', \n",
    "                                      'population').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create an UDF for a column that calculates the score per record\n",
    "\n",
    "def get_cultural_score(location_type, count, population):\n",
    "    cultural_weight_lookup = { \n",
    "        u'museum':      1.0,\n",
    "        u'arts_centre': 2.0,\n",
    "        u'theatre':     3.0,\n",
    "        u'gallery':     4.0,\n",
    "        u'artwork':     5.0  # try modifying the weights as an exercise\n",
    "    }\n",
    "\n",
    "    wgt = cultural_weight_lookup.get(location_type, 0.0)\n",
    "\n",
    "    return float((wgt* float(count) * 100000)/float(population))\n",
    "\n",
    "sqlContext.registerFunction(\"get_cultural_score\", get_cultural_score, FloatType())\n",
    "\n",
    "# score_udf = func.udf(get_cultural_score, FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# score_df = df.select(df.boundry_id, \n",
    "#           df.city_name, \n",
    "#           df.population,\n",
    "#           df.location_type,\n",
    "#           df.count,\n",
    "#           score_udf(df.location_type, df.count, df.population).alias('cultural_score')\n",
    "#          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.registerTempTable(\"cultural_score\")\n",
    "\n",
    "score_df = sqlContext.sql(\n",
    "    \"SELECT boundry_id, \\\n",
    "        city_name, \\\n",
    "        location_type, \\\n",
    "        population, \\\n",
    "        count, get_cultural_score(location_type, count, population) as score \\\n",
    "    FROM cultural_score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score_df.sort(score_df.city_name.asc()).show()\n",
    "# score_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = score_df.groupBy(\"boundry_id\", \n",
    "                      \"city_name\", \n",
    "                      \"population\").agg(func.sum(score_df.score)).sort(\"city_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df= df.withColumnRenamed(\"sum(score)\",\"final_score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd = df.toPandas()\n",
    "pd.sort_values(\"final_score\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the data\n",
    "\n",
    "Let map the data using the Folium package that allows embedding Leaflet Maps inside ipython notebooks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "pd_df = df_with_pois_bdrys.toPandas()\n",
    "pd_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd_df = pd_df[['boundry_id', 'boundry_wkt', 'city_name', 'population']]\n",
    "pd_df = pd_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "geometry = [wkt.loads(boundry_wkt) for boundry_wkt in pd_df.boundry_wkt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "geodf = GeoDataFrame(pd_df, geometry=geometry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "geodf.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores = pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores.head()\n",
    "scores_merged_df=pandas.DataFrame.merge(geodf, scores, on='boundry_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores_merged_df.head(1)\n",
    "geo_scores_merged = GeoDataFrame(scores_merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "geo_scores_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import folium\n",
    "map_osm = folium.Map(location=[47.19094, 11.98566], \n",
    "    tiles='Mapbox Bright',\n",
    "    zoom_start=6)\n",
    "\n",
    "map_osm.choropleth(geo_str=geo_scores_merged.to_json(),\n",
    "              data=geo_scores_merged,\n",
    "              columns=['city_name_x', 'final_score'],\n",
    "              fill_color='RdBu',\n",
    "              key_on='properties.city_name_x')\n",
    "map_osm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "toc": {
   "toc_cell": false,
   "toc_number_sections": true,
   "toc_threshold": 6,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
