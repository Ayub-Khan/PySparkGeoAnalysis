{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL is nice but...\n",
    "\n",
    "Most of the time in Spark SQL you can use Strings (in SQL) to reference columns but there are two cases where  you’ll want to use the Column objects rather than Strings :\n",
    "\n",
    "* In Spark SQL DataFrame columns are allowed to have the same name, they’ll be given unique names inside of Spark SQL, but this means that you can’t reference them with the column name only as this becomes ambiguous.\n",
    "\n",
    "* When you need to manipulate columns using expressions like Adding two columns to each other, Twice the value of this column or even Is the column value larger than 0?, you won’t be able to use simple strings and will need the Column reference.\n",
    "\n",
    "* Finally if you need renaming, cast or any other complex feature, you’ll need the Column reference too.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joins, UDFs, Broadcast Variables, Spatial Predicate\n",
    "\n",
    "## Joins\n",
    "Joining data is an important part of many of our pipelines, and both Spark core and Spark SQL support the same fundamental types of joins. While joins are very common and powerful, they warrant special performance consideration as they may require large network transfers or even create data sets beyond our capability to handle. In core Spark it can be more important to think about the ordering of operations, since the DAG optimizer, unlike the SQL optimizer isn’t able to re-order or push down filters.\n",
    "\n",
    "## User-Defined Functions (aka UDF)\n",
    "\n",
    "User-Defined Functions (aka UDF) is a feature of Spark SQL to define new Column-based functions that extend the vocabulary of Spark SQL’s DSL to transform Datasets.\n",
    "\n",
    "## Broadcast variables \n",
    "\n",
    "Broadcast variables allow the programmer to keep a read-only variable cached on each machine rather than shipping a copy of it with tasks. Explicitly creating broadcast variables is only useful when tasks across multiple stages need the same data or when caching the data in deserialized form is important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: for each city count the number of museums\n",
    "\n",
    "In this Exercise for each city we will count the number of museums and return a Dataframe with: city_name, museum_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load './code/helpers/imports.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load './code/helpers/load_boundaries_and_pois.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -------------------------------------- Soluntion below -------------------------------------- #\n",
    "\n",
    "# Hint: We want to do a join based on within sptial predicate (from shapely)\n",
    "# Hint: Get a dataframe with city name and city geometry\n",
    "# Hint: when you have the join perform a group by\n",
    "\n",
    "# Some Useful references to documentation\n",
    "# # http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.join\n",
    "# # http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.SQLContext.registerFunction\n",
    "# # http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.udf\n",
    "# # http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Column.cast\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Exercise\n",
    "# for each city count the number of museums\n",
    "# and return a DF with:\n",
    "# city_name, museum_count\n",
    "\n",
    "# SQL VErsion\n",
    "# cities_df = sqlContext.sql(\n",
    "#     \"\"\"\n",
    "#     SELECT properties.NAMEASCII AS city_name, \n",
    "#         geometry AS city_geom FROM boundaries\n",
    "#     \"\"\")\n",
    "\n",
    "cities_df = boundaries_from_pd.select(\n",
    "    (boundaries_from_pd.NAMEASCII).alias('city_name'),\n",
    "    (boundaries_from_pd.wkt).alias('city_geom'))\n",
    "\n",
    "cities_df.cache()\n",
    "\n",
    "# Create a broadcast variable\n",
    "# Broadcast http://spark.apache.org/docs/latest/programming-guide.html#broadcast-variables\n",
    "# Broadcast variables allow the programmer to keep a read-only variable cached on each machine \n",
    "# rather than shipping a copy of it with tasks. They can be used, for example, to give every node \n",
    "# a copy of a large input dataset in an efficient manner. Spark also attempts to distribute broadcast\n",
    "# variables using efficient broadcast algorithms to reduce communication cost.\n",
    "\n",
    "# Spark actions are executed through a set of stages, separated by distributed “shuffle” \n",
    "# operations. Spark automatically broadcasts the common data needed by tasks within each stage. \n",
    "# The data broadcasted this way is cached in serialized form and deserialized before running each task. \n",
    "# This means that explicitly creating broadcast variables is only useful when tasks across multiple \n",
    "# stages need the same data or when caching the data in deserialized form is important.\n",
    "\n",
    "# Broadcast variables are created from a variable v by calling SparkContext.broadcast(v). The broadcast \n",
    "# variable is a wrapper around v, and its value can be accessed by calling the value method.\n",
    "\n",
    "_cities_df = cities_df.toJSON().collect()[0:3]\n",
    "broadcastCitiesJSON = sc.broadcast(_cities_df)\n",
    "\n",
    "wkt.loads(json.loads(broadcastCitiesJSON.value[0])['city_geom'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_city_name(poi_geom):\n",
    "    # get an array of dict [(city_name, city_geom)]\n",
    "    cities = map(lambda c: {\n",
    "                    'city_name': json.loads(c)['city_name'],\n",
    "                    'city_wkt': wkt.loads(json.loads(c)['city_geom'])\n",
    "                }, broadcastCitiesJSON.value)\n",
    "\n",
    "    shply_poi = shape(poi_geom.asDict())\n",
    "    city = filter(lambda city: shply_poi.within(city['city_wkt']), cities)\n",
    "    name = None\n",
    "    if city:\n",
    "        name = city[0]['city_name']\n",
    "    return name\n",
    "\n",
    "# http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.SQLContext.registerFunction\n",
    "# Registers a python function (including lambda function) as a UDF so it can be used in SQL statements.\n",
    "# In addition to a name and the function itself, the return type can be optionally specified. When the \n",
    "# return type is not given it default to a string and conversion will automatically be done. For any other \n",
    "# return type, the produced object must match the specified type.\n",
    "\n",
    "sqlContext.udf.register(\"get_city_name\", get_city_name, StringType())\n",
    "\n",
    "# http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.udf\n",
    "# Creates a Column expression representing a user defined function (UDF).    \n",
    "get_city_name_udf = func.udf(get_city_name, StringType())\n",
    "\n",
    "# SQL VERSION\n",
    "# museums_df = sqlContext.sql(\n",
    "#     \"SELECT geometry as museum_geom, \\\n",
    "#     properties.name as museum_name, \\\n",
    "#     get_city_name(geometry) as city_name \\\n",
    "#     FROM pois WHERE properties.tourism = 'museum'\")\n",
    "\n",
    "\n",
    "museums_df = pois_df.select(\n",
    "    (pois_df.geometry).alias('museum_geom'),\n",
    "    (pois_df.properties.name).alias('museum_name'),\n",
    "    (get_city_name_udf(pois_df.geometry).alias('city_name'))\n",
    ")\n",
    "\n",
    "museums_df.registerTempTable(\"museums\")\n",
    "                       \n",
    "print museums_df.count()\n",
    "print cities_df.count()\n",
    "museums_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "museums_df.cache() # Try without and with\n",
    "print museums_df.where(museums_df.city_name.isNotNull()).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding population column\n",
    "According to our algorithm we will have to divide count by the the population to scale it per capita. Lets try to run our algorithm on a subset of the data to get practice for the real deal!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# lets recreate the cities DF this time including the population\n",
    "# http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Column.cast\n",
    "# Convert the column into type dataType\n",
    "cities_df = boundaries_from_pd.select(\n",
    "    (boundaries_from_pd.NAMEASCII).alias('city_name'),\n",
    "    (boundaries_from_pd.POPEU2013.cast(IntegerType())).alias('population'),\n",
    "    (boundaries_from_pd.wkt).alias('city_geom'))\n",
    "\n",
    "cities_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "museums_df = museums_df.dropna()\n",
    "\n",
    "# http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.join\n",
    "# Joins with another DataFrame, using the given join expression.\n",
    "\n",
    "# The following performs a full outer join between df1 and df2.\n",
    "\n",
    "# Parameters:\n",
    "# other – Right side of the join\n",
    "# on – a string for join column name, a list of column names, , a join expression (Column) or a list of Columns.\n",
    "#      If on is a string or a list of string indicating the name of the join column(s), the column(s) must exist \n",
    "#      on both sides, and this performs an equi-join.\n",
    "# how – str, default ‘inner’. One of inner, outer, left_outer, right_outer, leftsemi.\n",
    "\n",
    "df = museums_df.join(cities_df, museums_df.city_name == cities_df.city_name).select(\n",
    "    museums_df.museum_geom,\n",
    "    museums_df.museum_name,    \n",
    "    museums_df.city_name,\n",
    "    cities_df.population,\n",
    "    cities_df.city_geom\n",
    ")\n",
    "\n",
    "df.printSchema( )\n",
    "# Love Spark!\n",
    "\n",
    "grouped_by_city = df.groupBy('city_name', 'population')\n",
    "grouped_by_city = grouped_by_city.count()\n",
    "grouped_by_city.printSchema()\n",
    "grouped_by_city.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------ End of Exercise -------------------------------------------- #\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the score\n",
    "\n",
    "Now that we have the population and count of museums for each city - it should be possible to workout our score. Lets first rename the count column to something more useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.withColumnRenamed\n",
    "\n",
    "grouped_by_city = grouped_by_city.withColumnRenamed('count', 'museum_count')\n",
    "\n",
    "cultural_weight_lookup = {\n",
    "    'museum': 1, \n",
    "    'gallery': 2, \n",
    "    'artwork': 3 \n",
    "}\n",
    "\n",
    "# http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.withColumn\n",
    "# Returns a new DataFrame by adding a column or replacing the existing column that has the same name.\n",
    "\n",
    "# Parameters:\t\n",
    "# colName – string, name of the new column.\n",
    "# col – a Column expression for the new column.\n",
    "\n",
    "cultural_score_expression = (\n",
    "        grouped_by_city.museum_count/grouped_by_city.population\n",
    "    )*cultural_weight_lookup['museum']*10000\n",
    "\n",
    "grouped_by_city = grouped_by_city.withColumn('cultural_score', cultural_score_expression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grouped_by_city.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "toc": {
   "toc_cell": false,
   "toc_number_sections": true,
   "toc_threshold": 6,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
